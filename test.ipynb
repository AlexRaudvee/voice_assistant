{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3265, 0.9864, 0.8845],\n",
      "        [0.1297, 0.6591, 0.7476],\n",
      "        [0.1540, 0.9243, 0.3381],\n",
      "        [0.4882, 0.8818, 0.5418],\n",
      "        [0.9100, 0.6331, 0.4584]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexraudvee/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I can try.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I will.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I will try.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I think it's made of carbon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I think it's made of carbon.\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input('user: ') + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(f\"DialoGPT: {tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "# You can replace this embedding with your own as well.\n",
    "\n",
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pygame.mixer.Channel at 0x28da38b50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import os \n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"this file need to be here for normal functioning of the system.\"\n",
    "tts = gTTS(f\"{text}\", lang='ru')\n",
    "tts.save(\"out.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'fp' is not a file-like object or it does not take bytes: 'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/gtts/tts.py:307\u001b[0m, in \u001b[0;36mgTTS.write_to_fp\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mfor\u001b[39;00m idx, decoded \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream()):\n\u001b[0;32m--> 307\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(decoded)\n\u001b[1;32m    308\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mpart-\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m written to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, idx, fp)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb Ячейка 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mтеперь тут другой текст\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tts \u001b[39m=\u001b[39m gTTS(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mru\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tts\u001b[39m.\u001b[39;49mwrite_to_fp(\u001b[39m'\u001b[39;49m\u001b[39mmp3_fp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/gtts/tts.py:310\u001b[0m, in \u001b[0;36mgTTS.write_to_fp\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m    308\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mpart-\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m written to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, idx, fp)\n\u001b[1;32m    309\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not a file-like object or it does not take bytes: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    312\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'fp' is not a file-like object or it does not take bytes: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "text = \"теперь тут другой текст\"\n",
    "tts = gTTS(f\"{text}\", lang='ru')\n",
    "tts.write_to_fp('mp3_fp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from tempfile import TemporaryFile\n",
    "import pyglet\n",
    "from time import sleep\n",
    "\n",
    "tts = gTTS(text='сек', lang='ru', )\n",
    "tts.save(\"out.mp3\")\n",
    "music = pyglet.media.load(\"out.mp3\", streaming=False)\n",
    "music.play()\n",
    "\n",
    "sleep(music.duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 8.81k/8.81k [00:00<00:00, 3.04MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 4.49G/4.49G [02:12<00:00, 33.8MB/s]\n",
      "/Users/alexraudvee/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "generation_config.json: 100%|██████████| 4.91k/4.91k [00:00<00:00, 1.83MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 353/353 [00:00<00:00, 144kB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 2.93MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.92M/2.92M [00:00<00:00, 31.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 50.4kB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb Ячейка 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m synthesiser \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-to-speech\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msuno/bark\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m speech \u001b[39m=\u001b[39m synthesiser(\u001b[39m\"\u001b[39;49m\u001b[39mHello, my dog is cooler than you!\u001b[39;49m\u001b[39m\"\u001b[39;49m, forward_params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdo_sample\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m scipy\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mwavfile\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39mbark_out.wav\u001b[39m\u001b[39m\"\u001b[39m, rate\u001b[39m=\u001b[39mspeech[\u001b[39m\"\u001b[39m\u001b[39msampling_rate\u001b[39m\u001b[39m\"\u001b[39m], data\u001b[39m=\u001b[39mspeech[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/pipelines/text_to_audio.py:138\u001b[0m, in \u001b[0;36mTextToAudioPipeline.__call__\u001b[0;34m(self, text_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/pipelines/text_to_audio.py:112\u001b[0m, in \u001b[0;36mTextToAudioPipeline._forward\u001b[0;34m(self, model_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(kwargs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcan_generate():\n\u001b[0;32m--> 112\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:1600\u001b[0m, in \u001b[0;36mBarkModel.generate\u001b[0;34m(self, input_ids, history_prompt, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m semantic_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemantic\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1593\u001b[0m     input_ids,\n\u001b[1;32m   1594\u001b[0m     history_prompt\u001b[39m=\u001b[39mhistory_prompt,\n\u001b[1;32m   1595\u001b[0m     semantic_generation_config\u001b[39m=\u001b[39msemantic_generation_config,\n\u001b[1;32m   1596\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_semantic,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[1;32m   1599\u001b[0m \u001b[39m# 2. Generate from the coarse model\u001b[39;00m\n\u001b[0;32m-> 1600\u001b[0m coarse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoarse_acoustics\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1601\u001b[0m     semantic_output,\n\u001b[1;32m   1602\u001b[0m     history_prompt\u001b[39m=\u001b[39;49mhistory_prompt,\n\u001b[1;32m   1603\u001b[0m     semantic_generation_config\u001b[39m=\u001b[39;49msemantic_generation_config,\n\u001b[1;32m   1604\u001b[0m     coarse_generation_config\u001b[39m=\u001b[39;49mcoarse_generation_config,\n\u001b[1;32m   1605\u001b[0m     codebook_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mcodebook_size,\n\u001b[1;32m   1606\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_coarse,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[39m# 3. \"generate\" from the fine model\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_acoustics\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1611\u001b[0m     coarse_output,\n\u001b[1;32m   1612\u001b[0m     history_prompt\u001b[39m=\u001b[39mhistory_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_fine,\n\u001b[1;32m   1618\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:1012\u001b[0m, in \u001b[0;36mBarkCoarseModel.generate\u001b[0;34m(self, semantic_output, semantic_generation_config, coarse_generation_config, codebook_size, history_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m input_coarse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack(\n\u001b[1;32m    999\u001b[0m     [\n\u001b[1;32m   1000\u001b[0m         input_coarse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     ]\n\u001b[1;32m   1004\u001b[0m )\n\u001b[1;32m   1006\u001b[0m alternatingLogitsProcessor \u001b[39m=\u001b[39m AlternatingCodebooksLogitsProcessor(\n\u001b[1;32m   1007\u001b[0m     input_coarse\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m   1008\u001b[0m     semantic_generation_config\u001b[39m.\u001b[39msemantic_vocab_size,\n\u001b[1;32m   1009\u001b[0m     codebook_size,\n\u001b[1;32m   1010\u001b[0m )\n\u001b[0;32m-> 1012\u001b[0m output_coarse \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1013\u001b[0m     input_coarse,\n\u001b[1;32m   1014\u001b[0m     logits_processor\u001b[39m=\u001b[39;49m[alternatingLogitsProcessor],\n\u001b[1;32m   1015\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39mmin\u001b[39;49m(sliding_window_len, max_generated_len \u001b[39m-\u001b[39;49m total_generated_len),\n\u001b[1;32m   1016\u001b[0m     generation_config\u001b[39m=\u001b[39;49mcoarse_generation_config,\n\u001b[1;32m   1017\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[1;32m   1020\u001b[0m input_coarse_len \u001b[39m=\u001b[39m input_coarse\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1022\u001b[0m x_coarse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack([x_coarse, output_coarse[:, input_coarse_len:]])\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1720\u001b[0m         input_ids,\n\u001b[1;32m   1721\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1722\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1723\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1724\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1725\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1726\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1727\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1728\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1729\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1733\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2803\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2804\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2805\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2806\u001b[0m )\n\u001b[1;32m   2808\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:650\u001b[0m, in \u001b[0;36mBarkCausalModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, labels, input_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    640\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    641\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    642\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m         output_attentions,\n\u001b[1;32m    648\u001b[0m     )\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[1;32m    652\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_layer_key_values,\n\u001b[1;32m    653\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    654\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    655\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    656\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    657\u001b[0m     )\n\u001b[1;32m    659\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:247\u001b[0m, in \u001b[0;36mBarkBlock.forward\u001b[0;34m(self, hidden_states, past_key_values, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m ):\n\u001b[1;32m    245\u001b[0m     intermediary_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_1(hidden_states)\n\u001b[0;32m--> 247\u001b[0m     attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    248\u001b[0m         intermediary_hidden_states,\n\u001b[1;32m    249\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    250\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    251\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    252\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    253\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: output, present_key_values, (attn_weights)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:177\u001b[0m, in \u001b[0;36mBarkSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, past_key_values, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     present \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    179\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    180\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/models/bark/modeling_bark.py:146\u001b[0m, in \u001b[0;36mBarkSelfAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    142\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m    144\u001b[0m \u001b[39m# (batch, num_heads, seq_len, seq_len) x (batch, num_heads, seq_len, attn_head_size)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m# -> (batch, num_heads, seq_len, attn_head_size)\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn_weights, value)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\")\n",
    "\n",
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexraudvee/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/alexraudvee/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "preprocessor_config.json: 100%|██████████| 1.78k/1.78k [00:00<00:00, 1.00MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 19.5k/19.5k [00:00<00:00, 7.81MB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████| 5.17M/5.17M [00:00<00:00, 33.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 2.12k/2.12k [00:00<00:00, 1.20MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 1.04MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|██████████| 2.56k/2.56k [00:00<00:00, 1.39MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 9.44G/9.44G [04:09<00:00, 37.9MB/s]\n",
      "generation_config.json: 100%|██████████| 3.35k/3.35k [00:00<00:00, 380kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, SeamlessM4TModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
    "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "# let's load an audio sample from an Arabic speech corpus\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True)\n",
    "audio_sample = next(iter(dataset))[\"audio\"]\n",
    "\n",
    "# now, process it\n",
    "audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n",
    "\n",
    "# now, process some English test as well\n",
    "text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydub\n",
    "\n",
    "def write(f, sr, x, normalized=False):\n",
    "    \"\"\"numpy array to MP3\"\"\"\n",
    "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
    "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "        y = np.int16(x * 2 ** 15)\n",
    "    else:\n",
    "        y = np.int16(x)\n",
    "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
    "    song.export(f, format=\"mp3\", bitrate=\"320k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "write(\"out.mp3\", sr=44100, x=audio_array_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.8693016e-04, -3.3721735e-04, -3.6201600e-04, ...,\n",
       "       -4.2085419e-05, -5.7400321e-05, -2.1124724e-05], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb Ячейка 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline, Conversation\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspeech_recognition\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msr\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunc_\u001b[39;00m \u001b[39mimport\u001b[39;00m say, mic, r\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m chater \u001b[39m=\u001b[39m pipeline(model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfacebook/blenderbot-400M-distill\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/dev/voice_stuff/test.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdialog_with_AI\u001b[39m(start_message: \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/func_.py:104\u001b[0m\n\u001b[1;32m    102\u001b[0m say(\u001b[39m\"\u001b[39m\u001b[39mпривет черномазый\u001b[39m\u001b[39m\"\u001b[39m, lang)\n\u001b[1;32m    103\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     transcript \u001b[39m=\u001b[39m conversation(lang)\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m transcript \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mотмена\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    106\u001b[0m         say(\u001b[39m\"\u001b[39m\u001b[39mладно, я офф!\u001b[39m\u001b[39m\"\u001b[39m, lang)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/func_.py:76\u001b[0m, in \u001b[0;36mconversation\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mwith\u001b[39;00m mic \u001b[39mas\u001b[39;00m source:\n\u001b[1;32m     75\u001b[0m     r\u001b[39m.\u001b[39madjust_for_ambient_noise(source, duration\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     audio \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mlisten(source)\n\u001b[1;32m     77\u001b[0m     transcript \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrecognize_google(audio, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mru-RU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m transcript\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/speech_recognition/__init__.py:523\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[0;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m phrase_time_limit \u001b[39mand\u001b[39;00m elapsed_time \u001b[39m-\u001b[39m phrase_start_time \u001b[39m>\u001b[39m phrase_time_limit:\n\u001b[1;32m    521\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(source\u001b[39m.\u001b[39;49mCHUNK)\n\u001b[1;32m    524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mbreak\u001b[39;00m  \u001b[39m# reached end of the stream\u001b[39;00m\n\u001b[1;32m    525\u001b[0m frames\u001b[39m.\u001b[39mappend(buffer)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/speech_recognition/__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, size):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyaudio_stream\u001b[39m.\u001b[39;49mread(size, exception_on_overflow\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Desktop/dev/voice_stuff/.conda/lib/python3.9/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames,\n\u001b[1;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "import speech_recognition as sr\n",
    "\n",
    "from func_ import say, mic, r\n",
    "\n",
    "chater = pipeline(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "\n",
    "def dialog_with_AI(start_message: str):\n",
    "    while user != 'q':\n",
    "        user = start_message\n",
    "        conversation = Conversation(user)\n",
    "        conversation = chater(conversation)\n",
    "        say(conversation.generated_responses[-1])\n",
    "        \n",
    "        try:    \n",
    "            with mic as source:\n",
    "                r.adjust_for_ambient_noise(source, duration=0.5)\n",
    "                audio = r.listen(source)\n",
    "                user = r.recognize_google(audio, language=\"en-EN\")\n",
    "\n",
    "                \n",
    "                conversation.add_user_input(user)\n",
    "                conversation = chater(conversation)\n",
    "                say(conversation.generated_responses[-1])\n",
    "\n",
    "                r.adjust_for_ambient_noise(source, duration=0.5)\n",
    "                audio = r.listen(source)\n",
    "                user = r.recognize_google(audio, language=\"en-EN\")\n",
    "\n",
    "                conversation.add_user_input(user)\n",
    "                conversation = chater(conversation)\n",
    "                say(conversation.generated_responses[-1])\n",
    "\n",
    "                conversation = Conversation()\n",
    "        except sr.UnknownValueError:\n",
    "            say(\"i didn't hear you well can your repeat what you said?\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RetryProvider provider\n",
      "Using You provider\n",
      "You: RuntimeError: HTTP Error 403: \n",
      "Using GptGo provider\n",
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "import g4f\n",
    "\n",
    "g4f.debug.logging = True  # Enable logging\n",
    "g4f.check_version = False  # Disable automatic version checking\n",
    "# print(g4f.version)  # Check version\n",
    "# print(g4f.Provider.Ails.params)  # Supported args\n",
    "\n",
    "# Automatic selection of provider\n",
    "\n",
    "# Streamed completion\n",
    "response = g4f.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(message, flush=True, end=\"\")\n",
    "\n",
    "# # Normal response\n",
    "# response = g4f.ChatCompletion.create(\n",
    "#     model=g4f.models.gpt_4,\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "# )  # Alternative model setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.system(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
